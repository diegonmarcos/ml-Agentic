# VPS Provider Research & Pricing

**Last Updated**: 2025-11-19
**Purpose**: Compare VPS providers for cost-effectiveness and LLM deployment
**Focus Areas**: Free tiers, cheap options, GPU/VRAM availability, LLM-native platforms

---

## üéØ Research Criteria

### Must-Have
- Low cost or free tier
- Linux support (Ubuntu/Debian preferred)
- Sufficient RAM (1GB minimum)
- Reliable uptime (>99%)
- Docker support

### Nice-to-Have
- GPU/VRAM access for LLMs
- Pay-as-you-use billing
- LLM-native infrastructure
- API access
- S3-compatible storage

---

## üí∞ Free Tier Providers

### 1. Oracle Cloud (Current Choice)

**Always Free Tier** ‚≠ê BEST FREE OPTION
- **Compute**: 2x VM.Standard.E2.1.Micro (AMD) - 1GB RAM, 2 vCPUs each
- **OR**: 4x ARM Ampere A1 cores + 24GB RAM (configurable)
- **Storage**: 200 GB Block Storage
- **Transfer**: 10 TB outbound/month
- **Duration**: **Permanent** (no time limit)
- **Cost**: $0/month forever

**Pros**:
- ‚úÖ Best free tier (no expiration)
- ‚úÖ ARM option excellent for LLM inference
- ‚úÖ Generous bandwidth (10 TB)
- ‚úÖ Multiple regions (EU, US, Asia)
- ‚úÖ Full VCN with load balancer

**Cons**:
- ‚ùå Account creation can be challenging
- ‚ùå Free tier resources can be reclaimed if inactive
- ‚ùå Limited support (community only)
- ‚ùå No GPU in free tier

**LLM Capability**:
- ARM A1 instance (4 cores, 24GB) can run small LLMs (7B models with quantization)
- Sufficient for llama.cpp, Ollama with GGUF models

**Current Status**: ‚úÖ Active (using AMD instance)

---

### 2. Google Cloud Platform (GCP)

**Free Tier**
- **Compute**: 1x e2-micro (0.25-0.5 vCPU, 1GB RAM)
- **Storage**: 30 GB HDD
- **Transfer**: 1 GB North America egress/month
- **Duration**: Permanent + $300 credit (90 days)
- **Cost**: $0/month (with limits)

**Pros**:
- ‚úÖ $300 free credit for 90 days
- ‚úÖ Excellent documentation
- ‚úÖ TPU access (paid)
- ‚úÖ Vertex AI for LLMs

**Cons**:
- ‚ùå Very limited free tier compute
- ‚ùå 1 GB bandwidth is restrictive
- ‚ùå TPU/GPU very expensive
- ‚ùå Can be complex to configure

**LLM Capability**:
- e2-micro too small for LLM hosting
- Vertex AI charges per-request (expensive)
- TPU Pods for training (starting $4.50/hour)

---

### 3. AWS (Amazon Web Services)

**Free Tier** (12 months)
- **Compute**: 750 hours/month t2.micro (1 vCPU, 1GB RAM)
- **Storage**: 30 GB EBS
- **Transfer**: 15 GB outbound
- **Duration**: 12 months only
- **Cost**: $0/month (first year), then ~$10/month

**Pros**:
- ‚úÖ Industry standard
- ‚úÖ Extensive service catalog
- ‚úÖ SageMaker for LLMs
- ‚úÖ Bedrock for managed LLMs

**Cons**:
- ‚ùå Only 12 months free
- ‚ùå Complex pricing (easy to overspend)
- ‚ùå GPU instances very expensive (p3.2xlarge ~$3/hour)
- ‚ùå Limited free tier

**LLM Capability**:
- AWS Bedrock: Pay-per-token (Claude, Llama2)
- SageMaker: Custom model deployment ($0.05/hour minimum)
- EC2 GPU: p3 instances starting $3.06/hour

---

### 4. Azure (Microsoft)

**Free Tier** (12 months)
- **Compute**: 750 hours/month B1S (1 vCPU, 1GB RAM)
- **Storage**: 64 GB + 5 GB blob
- **Transfer**: 15 GB outbound
- **Duration**: 12 months + $200 credit (30 days)
- **Cost**: $0/month (first year)

**Pros**:
- ‚úÖ $200 credit for experimentation
- ‚úÖ Azure OpenAI Service
- ‚úÖ Good Windows support
- ‚úÖ Strong enterprise features

**Cons**:
- ‚ùå Only 12 months free
- ‚ùå Complex portal
- ‚ùå GPU very expensive
- ‚ùå Limited free tier post-trial

**LLM Capability**:
- Azure OpenAI: Pay-per-token (GPT-4, GPT-3.5)
- Azure ML: Custom models ($0.10/hour minimum)
- NC-series GPU: Starting $0.90/hour (cheapest GPU option)

---

## üíµ Ultra-Cheap VPS Providers (No GPU)

### 5. Hetzner

**Cloud VPS**
- **Instance**: CX11 (1 vCPU, 2GB RAM, 20GB SSD)
- **Cost**: **‚Ç¨4.15/month** (~$4.50)
- **Transfer**: 20 TB included
- **Location**: Germany, Finland, USA

**Pros**:
- ‚úÖ Excellent price/performance
- ‚úÖ High bandwidth
- ‚úÖ IPv6 included
- ‚úÖ Good reputation
- ‚úÖ Simple pricing

**Cons**:
- ‚ùå No free tier
- ‚ùå No GPU options
- ‚ùå EU-focused (GDPR strict)

**LLM Capability**:
- Dedicated GPU servers available (not VPS)
- GPU server starting ‚Ç¨39/month (older GPUs)
- Can run small LLMs (7B quantized) on CX21 (2GB RAM)

---

### 6. Contabo

**VPS S SSD**
- **Instance**: 4 vCPUs, 8GB RAM, 200GB SSD
- **Cost**: **‚Ç¨5.99/month** (~$6.50)
- **Transfer**: 32 TB included
- **Location**: EU, USA, Asia

**Pros**:
- ‚úÖ Incredible RAM for price
- ‚úÖ Huge bandwidth
- ‚úÖ Multiple locations
- ‚úÖ Simple setup

**Cons**:
- ‚ùå Shared resources (oversubscribed)
- ‚ùå No GPU
- ‚ùå Mixed reviews on support
- ‚ùå Performance can vary

**LLM Capability**:
- 8GB RAM can run 7B models with quantization
- Good for Ollama, llama.cpp
- CPU-only inference (slow)

---

### 7. Vultr

**Regular Performance**
- **Instance**: 1 vCPU, 1GB RAM, 25GB SSD
- **Cost**: **$6/month**
- **Transfer**: 2 TB included
- **Location**: 25+ locations worldwide

**Cloud GPU** (Pay-as-you-use)
- **Instance**: 1x A100 (80GB VRAM)
- **Cost**: **$2.50/hour** (pay-as-you-use)
- **Also**: RTX 6000 Ada ($1.25/hour), A40 ($1.50/hour)

**Pros**:
- ‚úÖ Hourly GPU billing (no long commitment)
- ‚úÖ A100 available
- ‚úÖ Simple pricing
- ‚úÖ Good global coverage
- ‚úÖ Instant deployment

**Cons**:
- ‚ùå GPU expensive for 24/7 usage
- ‚ùå CPU VPS pricier than competitors
- ‚ùå Lower specs for price

**LLM Capability**: ‚≠ê EXCELLENT
- A100 80GB can run 70B models
- RTX 6000 Ada (48GB) for 33B models
- Pay-as-you-use perfect for inference spikes
- Pre-installed CUDA drivers

---

## üöÄ LLM-Native Platforms (Pay-as-you-use VRAM)

### 8. RunPod ‚≠ê BEST FOR LLMS

**Serverless GPU**
- **Pricing**: Pay per second of GPU usage
- **GPUs Available**:
  - RTX 4090 (24GB): **$0.69/hour**
  - RTX 4080 (16GB): **$0.49/hour**
  - RTX A6000 (48GB): **$0.89/hour**
  - A100 80GB: **$2.29/hour**
  - H100 80GB: **$4.89/hour**
- **Billing**: Per-second (minimum 5 seconds)
- **Autoscale**: 0 to 100+ GPUs instantly

**Pros**:
- ‚úÖ TRUE pay-per-second (sleep = $0)
- ‚úÖ Cheapest GPU pricing
- ‚úÖ LLM-optimized templates (vLLM, Text Generation Inference)
- ‚úÖ Jupyter, SSH, HTTP endpoints
- ‚úÖ Huge GPU selection
- ‚úÖ Automatic scaling

**Cons**:
- ‚ùå Cold start latency (5-30 seconds)
- ‚ùå Limited CPU-only options
- ‚ùå Network storage costs extra

**LLM Capability**: ‚≠ê‚≠ê‚≠ê EXCEPTIONAL
- Pre-built templates for Llama2, Mistral, Falcon
- vLLM for fast inference
- Can run 70B models on A100
- Autoscale based on demand

**Best Use Case**: Inference spikes, development, fine-tuning

**Cost Example**:
- Run 7B model 24/7 on RTX 4080: $354/month
- Run 7B model 8 hours/day: $118/month
- On-demand usage: $0.50/hour only when active

---

### 9. Vast.ai ‚≠ê CHEAPEST GPU

**Peer-to-Peer GPU Marketplace**
- **Pricing**: Varies by provider (auction-style)
- **Typical Prices**:
  - RTX 3090 (24GB): **$0.20-0.35/hour**
  - RTX 4090 (24GB): **$0.40-0.60/hour**
  - A100 40GB: **$0.80-1.20/hour**
  - A100 80GB: **$1.50-2.00/hour**
- **Billing**: Per hour (can stop anytime)

**Pros**:
- ‚úÖ CHEAPEST GPU prices (50-70% cheaper than clouds)
- ‚úÖ Huge GPU selection (consumer + data center)
- ‚úÖ Pay-as-you-go
- ‚úÖ Jupyter, SSH access
- ‚úÖ Docker support

**Cons**:
- ‚ùå Variable reliability (peer-hosted)
- ‚ùå Some hosts may disconnect
- ‚ùå No SLA/uptime guarantee
- ‚ùå Data transfer costs variable
- ‚ùå Need to find available instances

**LLM Capability**: ‚≠ê‚≠ê‚≠ê EXCELLENT (if reliable host)
- Can run 70B models on cheap A100
- RTX 3090 great for 13B-30B models
- Popular for ML training/inference

**Best Use Case**: Batch processing, experimentation, fine-tuning

---

### 10. Together.ai

**Serverless LLM Inference**
- **Pricing**: Per-token, not per-second
- **Models**:
  - Llama-2-70B: **$0.90/million tokens**
  - Mistral-7B: **$0.20/million tokens**
  - Llama-2-13B: **$0.23/million tokens**
- **Custom Models**: Bring your own (BYOM)
- **Fine-tuning**: Available

**Pros**:
- ‚úÖ No infrastructure management
- ‚úÖ Pay per token (very granular)
- ‚úÖ Fast inference (vLLM backend)
- ‚úÖ Can deploy custom models
- ‚úÖ API-first

**Cons**:
- ‚ùå No direct GPU access
- ‚ùå Limited to supported models
- ‚ùå Can be expensive for high volume

**LLM Capability**: ‚≠ê‚≠ê‚≠ê Serverless
- Optimized for inference
- No cold start
- Good for API-based applications

---

### 11. Lambda Labs

**On-Demand GPU Cloud**
- **Pricing**:
  - RTX 6000 Ada (48GB): **$0.80/hour**
  - A100 40GB: **$1.10/hour**
  - A100 80GB: **$1.99/hour**
  - H100 80GB: **$2.99/hour**
- **Billing**: Hourly, can stop anytime
- **Location**: USA primarily

**Pros**:
- ‚úÖ ML/AI focused
- ‚úÖ Pre-installed ML frameworks (PyTorch, TensorFlow)
- ‚úÖ Jupyter pre-configured
- ‚úÖ NVMe storage included
- ‚úÖ Good performance

**Cons**:
- ‚ùå Limited availability (popular GPUs often sold out)
- ‚ùå USA-focused
- ‚ùå No serverless autoscaling

**LLM Capability**: ‚≠ê‚≠ê‚≠ê Great for training
- Optimized for ML workloads
- Fast NVMe storage
- Multi-GPU support

---

### 12. Modal

**Serverless Python Functions + GPU**
- **Pricing**:
  - CPU: $0.0001/second
  - GPU (A100): **$0.0035/second** ($1.26/hour)
  - GPU (T4): **$0.0006/second** ($0.216/hour)
- **Billing**: Per-second of actual usage
- **Autoscale**: Automatic (0 to thousands)

**Pros**:
- ‚úÖ TRUE serverless (pay only when running)
- ‚úÖ Python-native (decorator-based)
- ‚úÖ Autoscaling to zero
- ‚úÖ Container-based
- ‚úÖ Great for API endpoints

**Cons**:
- ‚ùå Python only
- ‚ùå Not for long-running processes
- ‚ùå Cold start latency

**LLM Capability**: ‚≠ê‚≠ê‚≠ê Perfect for API inference
- Deploy LLM as API endpoint
- Auto-scale based on demand
- Sleep when idle (save $)

---

## üìä Comparison Tables

### Free Tier Comparison

| Provider | vCPUs | RAM | Storage | Bandwidth | Duration | Cost |
|----------|-------|-----|---------|-----------|----------|------|
| **Oracle Cloud** | 2 (or 4 ARM) | 1-24 GB | 200 GB | 10 TB | ‚àû | $0 |
| Google Cloud | 0.5 | 1 GB | 30 GB | 1 GB | ‚àû | $0 |
| AWS | 1 | 1 GB | 30 GB | 15 GB | 12mo | $0 |
| Azure | 1 | 1 GB | 64 GB | 15 GB | 12mo | $0 |

**Winner**: Oracle Cloud (no contest)

---

### Cheap CPU VPS Comparison

| Provider | vCPUs | RAM | Storage | Bandwidth | Cost/month |
|----------|-------|-----|---------|-----------|------------|
| Hetzner CX11 | 1 | 2 GB | 20 GB | 20 TB | **$4.50** |
| Contabo VPS S | 4 | 8 GB | 200 GB | 32 TB | **$6.50** |
| Vultr | 1 | 1 GB | 25 GB | 2 TB | $6 |
| DigitalOcean | 1 | 1 GB | 25 GB | 1 TB | $6 |
| Linode | 1 | 1 GB | 25 GB | 1 TB | $5 |

**Winner**: Contabo (most RAM/storage) or Hetzner (best reputation)

---

### Pay-as-you-use GPU Comparison (per hour)

| Provider | RTX 4090 (24GB) | A100 80GB | H100 80GB | Billing |
|----------|-----------------|-----------|-----------|---------|
| **RunPod** | **$0.69** | **$2.29** | **$4.89** | Per-second |
| **Vast.ai** | **$0.40-0.60** | **$1.50-2.00** | N/A | Per-hour |
| Lambda Labs | N/A | $1.99 | $2.99 | Per-hour |
| Vultr | N/A | $2.50 | N/A | Per-hour |
| Modal | N/A | $1.26 | N/A | Per-second |
| AWS (p4d) | N/A | ~$32* | N/A | Per-hour |

*AWS A100 = p4d.24xlarge (8x A100) = $32.77/hour

**Winner**: Vast.ai (cheapest) or RunPod (best reliability + per-second billing)

---

### LLM-Native Platform Comparison

| Provider | Model Size | Pricing Model | Cold Start | Best For |
|----------|-----------|---------------|------------|----------|
| RunPod | Up to 70B | Per-second GPU | 5-30s | Inference API |
| Vast.ai | Up to 70B | Per-hour GPU | 1-5min | Batch jobs |
| Together.ai | Hosted models | Per-token | None | API-only |
| Modal | Up to 70B | Per-second | 5-20s | Python functions |
| Replicate | Hosted models | Per-second | None | No-code inference |

---

## üí° Recommendations

### For Current Setup (Email + Sync + Matomo)
‚úÖ **Stick with Oracle Cloud Free Tier**
- Already deployed
- Sufficient resources
- No cost
- Can upgrade to ARM A1 for LLM later

---

### For LLM Development/Testing
üöÄ **RunPod or Modal**

**RunPod**:
- Best for self-hosted open source LLMs
- Pay per-second (sleep = $0)
- Easy to deploy (Docker templates)
- RTX 4090 at $0.69/hour

**Modal**:
- Best for Python-based LLM APIs
- Serverless autoscaling
- Deploy with `@app.function(gpu="a100")`
- Great for occasional inference

---

### For Production LLM Inference (24/7)
üí∞ **Cheap CPU VPS (Hetzner) + API calls (Together.ai)**

**Why**:
- Run orchestration on Hetzner ($4.50/month)
- Call Together.ai API for LLM inference ($0.20-0.90/million tokens)
- Much cheaper than 24/7 GPU server
- No infrastructure management

**Cost Example**:
- Hetzner VPS: $4.50/month
- 10M tokens/month: $9 (Together.ai Mistral-7B)
- **Total**: ~$13.50/month

**vs. 24/7 GPU**:
- RunPod RTX 4090 24/7: $497/month
- Vast.ai RTX 3090 24/7: ~$172/month

---

### For Heavy LLM Training/Fine-tuning
‚ö° **Vast.ai (cheapest) or Lambda Labs (reliability)**

**Vast.ai**:
- RTX 3090 at $0.20-0.35/hour
- A100 80GB at $1.50-2.00/hour
- Perfect for batch jobs
- Save 50-70% vs. AWS/Azure

**Lambda Labs**:
- More reliable than Vast.ai
- Better support
- A100 80GB at $1.99/hour
- Good for multi-day training runs

---

### For Experimentation (Learning LLMs)
üÜì **Oracle Cloud ARM A1 + Ollama**

**Setup**:
1. Create ARM A1 instance (4 cores, 24GB RAM) - FREE
2. Install Ollama
3. Run Llama-2-7B or Mistral-7B with quantization (GGUF)

**Pros**:
- Completely free
- Learn LLM inference
- Enough RAM for 7B models
- No time limit

**Performance**:
- CPU inference (slow but works)
- ~1-3 tokens/second on ARM
- Fine for experimentation

---

## üéØ Cost Optimization Strategies

### Strategy 1: Hybrid Cloud
- **Static services** (email, sync, web) ‚Üí Oracle Free Tier ($0)
- **LLM orchestration** ‚Üí Hetzner CX11 ($4.50/month)
- **LLM inference** ‚Üí Together.ai API (pay-per-token)
- **Occasional GPU** ‚Üí RunPod (per-second billing)

**Monthly Cost**: $5-20 depending on usage

---

### Strategy 2: All-In Oracle + API
- **Everything** ‚Üí Oracle Free Tier ARM A1 ($0)
- **Heavy LLM tasks** ‚Üí Together.ai or OpenRouter API
- **No GPU** (use external APIs)

**Monthly Cost**: $0-50 (API usage only)

---

### Strategy 3: GPU On-Demand
- **Base services** ‚Üí Oracle Free Tier ($0)
- **LLM inference** ‚Üí RunPod Serverless (per-second)
- **Autoscale to zero** when not in use

**Monthly Cost**: $0-100 (depending on inference load)

---

## üìù Provider Quick Links

### Free Tiers
- **Oracle Cloud**: https://www.oracle.com/cloud/free/
- **Google Cloud**: https://cloud.google.com/free
- **AWS**: https://aws.amazon.com/free/
- **Azure**: https://azure.microsoft.com/free/

### Cheap VPS
- **Hetzner**: https://www.hetzner.com/cloud
- **Contabo**: https://contabo.com/
- **Vultr**: https://www.vultr.com/pricing/
- **DigitalOcean**: https://www.digitalocean.com/pricing

### GPU Providers
- **RunPod**: https://www.runpod.io/pricing
- **Vast.ai**: https://vast.ai/pricing
- **Lambda Labs**: https://lambdalabs.com/service/gpu-cloud
- **Modal**: https://modal.com/pricing
- **Together.ai**: https://www.together.ai/pricing

---

## ‚ö†Ô∏è Important Notes

### Oracle Cloud Free Tier
- **Reclamation Risk**: Oracle may reclaim free tier resources if:
  - Account inactive for 90+ days
  - CPU utilization consistently <10%
  - Suspected abuse
- **Mitigation**: Run background cron job to keep CPU active

### Vast.ai Reliability
- Peer-to-peer marketplace = variable quality
- Check host reliability score (>98%)
- Use interruptible instances for batch jobs only
- Consider Lambda/RunPod for production

### GPU Pricing Volatility
- Prices change based on demand
- RunPod/Vast.ai prices fluctuate
- Lock in reserved instances for 24/7 workloads
- Monitor spot pricing for savings

---

**Research Version**: 1.0
**Next Update**: When significant pricing changes occur
**Maintained By**: Claude Code
