<!--
Sync Impact Report
==================
Version: 1.0.0 → 2.0.0 (Major Architecture Update)
Modified Principles: II (LLM-Agnostic), IV (Cost-Conscious), V (Observable)
Added Sections: VI (Intelligent Routing), VII (Tool Integration Standards), Observability Stack
Removed Sections: N/A
Breaking Changes: New unified LLM interface layer (aisuite), intelligent routing system (RouteLLM)
Templates Status: ✅ Requires update to spec, plan, research, and tasks
Follow-up TODOs: Integrate aisuite, RouteLLM, MCPcat, OpenTelemetry, Helicone, Firecrawl, PageIndex MCP
-->

# n8n LLM Orchestrator Constitution

## Core Principles

### I. Visual-First Design
All agentic workflows MUST be representable in n8n's visual interface. This ensures:
- Non-technical stakeholders can understand agent flows at a glance
- Debugging is visual and intuitive
- No hidden logic in external scripts unless absolutely necessary for performance
- Complex multi-agent orchestrations remain maintainable

**Rationale**: Visual workflows reduce cognitive load and make agentic systems accessible to broader teams.

### II. LLM-Agnostic Architecture
The system MUST support multiple LLM providers interchangeably:
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude 3.5 Sonnet, Claude 3 Opus)
- Google (Gemini Pro, Gemini Flash)
- Ollama (local models: Llama 3.1, Mistral, Qwen)

Requirements:
- ALL LLM calls MUST go through unified interface layer (**aisuite**)
- Provider switching MUST NOT require workflow redesign
- Each agent can specify preferred provider via configuration
- Fallback chains supported (e.g., Ollama → Claude → GPT-4)
- n8n workflows interact with aisuite HTTP API, not individual providers

**Rationale**: Avoid vendor lock-in, enable cost optimization, support offline/local operation. Single API interface reduces n8n complexity and maintenance burden.

### III. Fail-Safe Operations (NON-NEGOTIABLE)
Every LLM call, tool execution, and data retrieval MUST implement error handling:
- Automatic retries with exponential backoff (max 3 attempts)
- Graceful degradation when services unavailable
- Meaningful error messages returned to user
- Circuit breaker pattern for repeated failures
- No workflow crash—always return actionable response

**Rationale**: LLM APIs are inherently unreliable; production systems require resilience.

### IV. Cost-Conscious Design
Token usage and API costs MUST be actively managed:
- Monitor and log token consumption per workflow via **Helicone** proxy
- Track real-time costs with **MCPcat** production monitoring
- Implement prompt optimization strategies (chunking, compression)
- Cache LLM responses for identical queries via **Helicone** (1-hour TTL)
- Prefer local Ollama for development and high-volume use cases
- Use **RouteLLM** to automatically route simple queries to cheaper models
- Track monthly spend with budget alerts via **LangSmith** or **Langfuse**

**Rationale**: Unchecked LLM usage can lead to unexpected costs; proactive monitoring with professional tools prevents budget overruns. Intelligent routing optimizes cost-quality tradeoff automatically.

### V. Observable Agent Flows
Every agent step MUST be traceable and debuggable:
- Distributed tracing with **OpenTelemetry** across n8n → aisuite → providers
- Structured logging with context (agent name, step, timestamp) via **MCPcat**
- Capture all LLM prompts and responses in **LangSmith** or **Langfuse**
- Track RAG retrieval results (chunks retrieved, relevance scores)
- Execution metrics (latency, token count, success rate) via **Helicone**
- Full request traces with visualization in **LangSmith** dashboard
- n8n execution logs retained for 30 days minimum

**Rationale**: Complex multi-agent systems are opaque without comprehensive observability. Professional tools provide trace visualization and debugging capabilities that simple logging cannot match.

### VI. Intelligent Routing (NEW)
Query complexity MUST determine model selection automatically:
- **RouteLLM** analyzes queries and routes to appropriate model tier
- Simple queries (factual lookup, formatting) → Local Ollama or Gemini Flash
- Medium queries (reasoning, multi-step) → Claude 3.5 Sonnet or GPT-4
- Complex queries (advanced reasoning, coding) → Claude 3 Opus or GPT-4 Turbo
- Routing decisions logged and auditable
- Manual override available per workflow via configuration

**Rationale**: Manual model selection wastes time and money. Intelligent routing optimizes cost-quality tradeoff while maintaining answer quality. Reduces cognitive load on developers.

### VII. Tool Integration Standards (NEW)
External tools and data sources MUST follow MCP (Model Context Protocol) patterns:
- **Web scraping**: Use **Firecrawl** for LLM-friendly structured output
- **Documentation access**: Use **PageIndex MCP** for vectorless RAG (cost-efficient)
- **App integrations**: Use **Composio** or **Rube** for standardized API access
- **Authentication**: Use **AgentAuth** for OAuth flows and token management
- All tools MUST expose consistent interfaces via n8n HTTP Request nodes
- Tool failures MUST NOT crash workflows (fallback to degraded functionality)

**Rationale**: Standardized tool integration reduces maintenance burden and enables tool swapping. MCP provides proven patterns for context management.

## Architecture Constraints

### Vector Database Standards
- **Primary**: Qdrant (self-hosted) for production RAG workflows with embeddings
- **Alternative (Cost-Optimized)**: PageIndex MCP for vectorless RAG on documentation
- **Alternative (Cloud)**: Supabase Vector for cloud-hosted scenarios
- All embeddings MUST use consistent model (nomic-embed-text, 768 dimensions)
- Collections MUST include metadata: source, timestamp, chunk_id, version
- Backup collections daily via Qdrant snapshot API
- Use PageIndex MCP for frequently-accessed documentation to reduce embedding costs

### Document Processing Rules
- Chunk size: 800 characters (±200 acceptable for specific use cases)
- Chunk overlap: 200 characters minimum
- Top-k retrieval: 3-5 chunks (never exceed 10 to prevent context overflow)
- Re-ranking optional but recommended for precision-critical applications

### State Management
- Agent state MUST be passed explicitly between n8n nodes (no hidden globals)
- Long-running workflows MUST persist state to prevent data loss
- Maximum workflow execution time: 5 minutes (configurable per workflow)

### LLM Interface & Routing Layer
- **Unified API**: All LLM calls routed through **aisuite** HTTP wrapper service
- **Intelligent Routing**: **RouteLLM** determines optimal model based on query complexity
- **Cost Proxy**: **Helicone** sits between aisuite and providers for caching/logging
- **Request Flow**: n8n → aisuite → RouteLLM → Helicone → Provider (Ollama/Claude/Gemini/OpenAI)
- **Configuration**: Model preferences, routing thresholds, fallback chains in environment variables
- **Override**: Manual model selection available via workflow parameters

### Observability Stack (Required for Production)
- **Distributed Tracing**: OpenTelemetry with Jaeger/Zipkin backend
- **LLM Monitoring**: LangSmith (preferred) or Langfuse (opensource alternative)
- **Cost Tracking**: Helicone proxy with real-time dashboard
- **Agent Metrics**: MCPcat for tool call tracking and error monitoring
- **Integration**: All tools export traces to OpenTelemetry collector
- **Retention**: 30 days for execution logs, 90 days for aggregated metrics
- **Alerting**: Budget alerts, error rate thresholds, latency spikes

## Quality Standards

### Testing Requirements
- All RAG pipelines MUST have evaluation datasets with ground truth
- Measure and track: retrieval accuracy (recall@k), answer relevance, latency
- Integration tests required for: document ingestion, query workflows, fallback paths
- Manual testing protocol documented for each workflow before production

### Performance Benchmarks
- RAG query response time: < 5 seconds end-to-end
- Document ingestion: > 10 docs/minute
- Concurrent workflow support: minimum 5 simultaneous executions
- LLM response timeout: 30 seconds (configurable)

### Security
- API keys stored in n8n credentials (never hardcoded)
- Vector database access restricted to local network or VPN
- User input sanitized before LLM prompts
- No PII in logs unless explicitly required and compliant

## Governance

This constitution supersedes all other development practices. Changes require:
1. Proposal with rationale and impact analysis
2. Version bump following semantic versioning (MAJOR.MINOR.PATCH)
3. Update to all dependent templates and documentation
4. Team review (or author review for solo projects)

All workflows and implementations MUST be verified against these principles during:
- Design review (before implementation)
- Code/workflow review (before deployment)
- Post-deployment audit (after 1 week in production)

Complexity that violates these principles MUST be explicitly justified and documented as technical debt with remediation plan.

**Version**: 2.0.0 | **Ratified**: 2025-10-30 | **Last Amended**: 2025-11-17
