# Architecture Constitution v4.0

**Version**: 4.0.0
**Date**: 2025-11-18
**Status**: Production Architecture - Multi-Agent Orchestrator
**Breaking Changes**: Yes - Pivot from RAG Q&A to Multi-Agent Coder + Web Agent System

---

## Preamble

This constitution defines the architectural principles for a **Multi-Agent Orchestrator** system that enables intelligent automation of coding tasks and web tasks through cost-optimized LLM routing. The system supports two specialized agents:

1. **Coder Agent**: Interacts with codebases (read, modify, commit)
2. **Web Agent**: Interacts with websites (scrape, automate, research)

Both agents use the same 5-tier cost optimization framework while specializing in their respective domains.

---

## Core Principles

### I. Visual-First Design

**Requirement**: All multi-agent workflows MUST be representable in n8n's visual interface.

**Rationale**: Visual workflows enable non-programmers to understand, debug, and modify agent behavior. Agent logic should be transparent, not hidden in code.

**Requirements**:
- ALL agent orchestration logic MUST be expressed as n8n workflows
- ALL branching decisions (agent selection, tier routing) MUST use n8n IF nodes
- ALL loops (task execution, file processing) MUST use n8n Loop nodes
- Custom code MUST be limited to n8n Code nodes (JavaScript only, < 50 lines per node)
- No external Python/Node.js services for core agent logic

**Agent-Specific**:
- **Coder Agent**: File read/write operations via n8n File nodes or Code nodes
- **Web Agent**: Browser automation via HTTP calls to Playwright service (n8n HTTP Request nodes)

**Implementation**:
- Master Orchestrator: n8n workflow with agent selection logic
- Coder Agent: n8n workflow with file I/O and git operations
- Web Agent: n8n workflow with Playwright HTTP calls

**Validation**:
- ✅ Can I export the entire agent logic as n8n JSON?
- ✅ Can I visualize the agent flow in n8n GUI?
- ✅ Can I test each node independently in n8n test mode?

---

### II. LLM-Agnostic & Modality-Aware Architecture

**Requirement**: Support multiple LLM providers interchangeably with automatic modality detection.

**Rationale**: Avoid vendor lock-in. Enable cost optimization by switching providers. Automatically route vision tasks to vision-capable models.

**Requirements**:
- ALL LLM calls MUST go through unified interface layer (**LiteLLM**, not direct API calls)
- Provider switching MUST NOT require workflow redesign
- **Modality detection**: Automatically select vision-capable models for screenshot analysis tasks
- **Provider aliasing**: Use model groups (architect, executor, vision) not specific models

**Agent-Specific**:
- **Coder Agent**: Text-only models sufficient (Tier 0/1/3)
- **Web Agent**: May need vision models (Tier 2) for screenshot analysis

**LiteLLM Configuration**:
```yaml
model_list:
  - model_name: fast_filter    # Tier 0: Classification, intent detection
    litellm_params:
      model: ollama/llama3.2:3b

  - model_name: executor       # Tier 1: Code generation, web parsing
    litellm_params:
      model: fireworks_ai/llama-v3p1-8b-instruct

  - model_name: vision         # Tier 2: Screenshot analysis
    litellm_params:
      model: ollama/llama3.2-vision:70b

  - model_name: architect      # Tier 3: Planning, review
    litellm_params:
      model: claude-3-5-sonnet-20241022

  - model_name: batch_gpu      # Tier 4: Batch codebase indexing
    litellm_params:
      model: openai/meta-llama/llama-3.1-405b-instruct
      api_base: ${RUNPOD_ENDPOINT}
```

**Validation**:
- ✅ Can I switch from Claude to Gemini by changing 1 line in config?
- ✅ Does system auto-detect screenshots and route to vision model?
- ✅ Are all providers accessed through LiteLLM (no direct API calls)?

---

### III. Fail-Safe Operations (NON-NEGOTIABLE)

**Requirement**: System MUST handle errors gracefully and never leave codebases or web sessions in broken states.

**Rationale**: Agent modifications to code or web interactions can have destructive side effects. System must be production-safe.

**Requirements**:
- **ALL** LLM calls MUST have timeout (30s)
- **ALL** LLM calls MUST have retry with exponential backoff (max 3 retries)
- **ALL** file modifications MUST be git-tracked (automatic commits)
- **ALL** web browser sessions MUST have cleanup handlers (close browser on error)
- **ALL** errors MUST return user-friendly messages (not stack traces)

**Agent-Specific Safeguards**:

**Coder Agent**:
- **Before modifying code**: Create git branch `ai-agent/{task-id}`
- **After modifications**: Automatic git commit with detailed message
- **On error**: Revert changes via `git reset --hard HEAD`
- **File operations**: Validate file exists before read, check permissions before write
- **Syntax validation**: Run linter/formatter after code generation (optional)

**Web Agent**:
- **Browser sessions**: Always run in headless mode with timeout
- **On error**: Screenshot saved to debug folder, browser closed
- **Authentication**: Never log credentials, use environment variables
- **Rate limiting**: Respect robots.txt, add delays between requests
- **Cleanup**: Always close browser on workflow end (try/finally pattern)

**Implementation**:
```javascript
// n8n Error Handling Pattern
try {
  // Execute agent task
  const result = await executeTask();

  // Git commit for Coder Agent
  if (agent === 'coder') {
    await gitCommit(result.modifiedFiles);
  }

  // Cleanup for Web Agent
  if (agent === 'web') {
    await closeBrowser();
  }

  return result;
} catch (error) {
  // Rollback for Coder Agent
  if (agent === 'coder') {
    await gitReset();
  }

  // Screenshot + cleanup for Web Agent
  if (agent === 'web') {
    await saveScreenshot(error.context);
    await closeBrowser();
  }

  // Return user-friendly error
  return {
    status: 'error',
    message: 'Task failed: ' + error.userMessage,
    details: error.debugInfo
  };
}
```

**Validation**:
- ✅ Does Coder Agent create git commit after every modification?
- ✅ Does Coder Agent revert on error?
- ✅ Does Web Agent close browser on error?
- ✅ Are all errors logged with full context?

---

### IV. Cost-Conscious Design

**Requirement**: Monitor token usage, cache responses, optimize prompts, **implement mandatory pre-budget checks**.

**Rationale**: Multi-agent workflows can quickly exhaust budgets through repeated LLM calls. Cost control is critical for production use.

**Requirements**:
- **PRE-BUDGET CHECK** (MANDATORY): Every LLM call MUST validate budget before execution
- **Token tracking**: Every LLM response MUST log token usage and cost
- **Caching**: Identical queries MUST be cached (Helicone 1-hour TTL)
- **Prompt optimization**: Long contexts MUST be compressed via Tier 0 before Tier 3
- **Batch detection**: High-volume tasks (>50 operations) MUST route to Tier 4 GPU rental

**Agent-Specific Optimization**:

**Coder Agent**:
- **Code retrieval**: Use RAG to retrieve only relevant files (not entire codebase)
- **Incremental processing**: Process one file at a time, not all files in batch
- **Architect/Executor pattern**: Plan with Tier 3, execute with Tier 0/1
- **Code context compression**: Send only function signatures, not full implementations

**Web Agent**:
- **Content extraction**: Use Firecrawl for structured extraction (cheaper than vision)
- **Vision fallback**: Only use Tier 2 vision when Firecrawl fails
- **Caching**: Cache web page content for 1 hour (avoid re-scraping)
- **Batch scraping**: If scraping >50 pages, route to Tier 4

**Pre-Budget Check Flow**:
```javascript
// MANDATORY before every LLM call
function preBudgetCheck(tier, estimatedTokens) {
  const pool = getTierBudgetPool(tier);
  const estimatedCost = calculateCost(tier, estimatedTokens);
  const currentBalance = getBudgetBalance(pool);

  if (currentBalance < estimatedCost) {
    // BLOCK the call
    throw new BudgetExhaustedError({
      pool: pool,
      balance: currentBalance,
      required: estimatedCost,
      message: 'Insufficient budget. Task blocked.'
    });
  }

  // Reserve budget (optimistic lock)
  reserveBudget(pool, estimatedCost);

  return {
    approved: true,
    pool: pool,
    estimatedCost: estimatedCost,
    balanceAfter: currentBalance - estimatedCost
  };
}
```

**Budget Pool Allocation**:
- Hourly VRAM (Tier 4): $100/month
- Per-Token (Tier 1): $50/month
- Premium (Tier 3): $50/month (increased from $30 due to more planning needed)
- Local VPS (Tier 0/2): $75/month

**Validation**:
- ✅ Does every LLM call run pre-budget check?
- ✅ Are all costs logged per agent task?
- ✅ Does system block calls when budget exhausted?
- ✅ Is prompt compression active for contexts >10k tokens?

---

### V. Observable Agent Flows

**Requirement**: Structured logging, prompt/response capture, metrics tracking, **cost analytics per agent**.

**Rationale**: Multi-agent systems are complex. Full observability is essential for debugging, cost optimization, and quality improvement.

**Requirements**:
- **ALL** workflow executions MUST log: start time, end time, status, agent type
- **ALL** LLM calls MUST log: prompt, response, tier used, cost, tokens
- **ALL** file modifications MUST log: file path, changes made, git commit hash
- **ALL** web interactions MUST log: URL, action, screenshot path, response time
- **Distributed tracing**: OpenTelemetry spans for full request path
- **Cost analytics**: Real-time dashboard with cost per agent, tier distribution, budget burn rate

**Agent-Specific Logging**:

**Coder Agent Logs**:
```json
{
  "agent": "coder",
  "task": "Refactor auth to JWT",
  "codebase": "/home/user/my-app",
  "execution_id": "exec_abc123",
  "trace_id": "trace_xyz789",
  "phases": [
    {
      "phase": "planning",
      "tier": 3,
      "model": "architect",
      "cost": 0.15,
      "tokens": 1250,
      "duration_ms": 3200,
      "output": "Plan with 7 subtasks"
    },
    {
      "phase": "execution",
      "subtasks": [
        {
          "task": "Find auth files",
          "tier": 0,
          "cost": 0,
          "files_found": ["src/auth.js", "middleware/jwt.js"]
        },
        {
          "task": "Generate JWT utils",
          "tier": 1,
          "cost": 0.035,
          "file_written": "src/utils/jwt.js",
          "lines_added": 45
        }
      ]
    },
    {
      "phase": "review",
      "tier": 3,
      "cost": 0.15,
      "validation": "passed",
      "suggestions": ["Add token expiry check"]
    },
    {
      "phase": "git_commit",
      "branch": "ai-agent/jwt-refactor",
      "commit_hash": "abc123def",
      "files_modified": 3
    }
  ],
  "total_cost": 0.335,
  "total_duration_ms": 15400,
  "status": "completed"
}
```

**Web Agent Logs**:
```json
{
  "agent": "web",
  "task": "Research competitor pricing",
  "target_url": "https://competitor.com/pricing",
  "execution_id": "exec_def456",
  "trace_id": "trace_uvw456",
  "phases": [
    {
      "phase": "planning",
      "tier": 3,
      "cost": 0.12,
      "plan": "Navigate, extract table, analyze"
    },
    {
      "phase": "web_automation",
      "browser_session_id": "browser_789",
      "actions": [
        {
          "action": "navigate",
          "url": "https://competitor.com/pricing",
          "duration_ms": 2100,
          "screenshot": "screenshots/pricing_page.png"
        },
        {
          "action": "extract_content",
          "method": "firecrawl",
          "content_length": 4500,
          "structured": true
        },
        {
          "action": "vision_analysis",
          "tier": 2,
          "cost": 0,
          "extracted": "3 pricing tiers identified"
        }
      ]
    },
    {
      "phase": "report_generation",
      "tier": 1,
      "cost": 0.05,
      "report_path": "research/competitor-pricing.md"
    }
  ],
  "total_cost": 0.17,
  "total_duration_ms": 8300,
  "status": "completed"
}
```

**Observability Stack**:
- **n8n Execution Logs**: All workflow I/O (30-day retention)
- **LangSmith/Langfuse**: All LLM prompts/responses (90-day retention)
- **OpenTelemetry + Jaeger**: Distributed traces (7-day retention)
- **Helicone**: Cost tracking, cache analytics (30-day retention)
- **MCPcat**: Tool call tracking (web automation actions)

**Cost Analytics Dashboard**:
- Cost per agent type (coder vs web)
- Cost per tier (0-4)
- Budget pool balances and burn rate
- Tier distribution (% queries per tier)
- Architect/executor cost breakdown
- Cache hit rate and savings

**Validation**:
- ✅ Can I see the full execution flow for any agent task?
- ✅ Can I calculate exact cost per agent invocation?
- ✅ Can I trace a request through all tiers and services?
- ✅ Does dashboard show real-time budget pool status?

---

### VI. Intelligent Routing

**Requirement**: Query/task complexity MUST determine tier selection automatically.

**Rationale**: Not all tasks need expensive Tier 3 models. Intelligent routing saves 60-70% cost while maintaining quality.

**5-Tier System**:

| Tier | Models | Cost | Latency | Use Cases |
|------|--------|------|---------|-----------|
| **0** | Ollama 3B-13B | $0 | <500ms | Classification, intent detection, simple code queries |
| **1** | Fireworks, Together | $0.20-0.80/M | 1-2s | Code generation, web parsing, standard tasks |
| **2** | Ollama 70B+Vision | $0 (VPS) | 2-5s | Screenshot analysis, image understanding |
| **3** | Claude, Gemini | $3-15/M | 3-8s | Planning, review, complex reasoning |
| **4** | RunPod, Salad GPU | $0.69-2/hr | 5-10s | Batch codebase indexing, bulk web scraping |

**Routing Logic**:

**Coder Agent**:
```javascript
function routeCoderTask(task, codebase) {
  // Tier 0: Simple code queries
  if (isSimpleQuery(task)) {
    // "What does this function do?"
    // "Find all API endpoints"
    return { tier: 0, model: 'fast_filter' };
  }

  // Tier 3: Planning phase (Architect)
  if (task.phase === 'planning') {
    return { tier: 3, model: 'architect' };
  }

  // Tier 1: Execution phase (Executor)
  if (task.phase === 'execution' && task.complexity < 0.7) {
    // "Generate utility function"
    // "Add logging statements"
    return { tier: 1, model: 'executor' };
  }

  // Tier 3: Complex execution or review
  if (task.phase === 'execution' && task.complexity >= 0.7) {
    // "Refactor to use dependency injection"
    return { tier: 3, model: 'architect' };
  }

  // Tier 3: Review phase (Reviewer)
  if (task.phase === 'review') {
    return { tier: 3, model: 'architect' };
  }

  // Tier 4: Batch indexing
  if (codebase.fileCount > 50 && task.type === 'indexing') {
    return { tier: 4, model: 'batch_gpu' };
  }

  // Default: Tier 1
  return { tier: 1, model: 'executor' };
}
```

**Web Agent**:
```javascript
function routeWebTask(task, content) {
  // Tier 0: Simple classification
  if (task.type === 'classify_content') {
    // "Is this a pricing page?"
    return { tier: 0, model: 'fast_filter' };
  }

  // Tier 3: Planning
  if (task.phase === 'planning') {
    return { tier: 3, model: 'architect' };
  }

  // Tier 2: Screenshot analysis
  if (content.hasScreenshot && !content.hasText) {
    return { tier: 2, model: 'vision' };
  }

  // Tier 1: Text extraction and parsing
  if (task.type === 'extract_data') {
    return { tier: 1, model: 'executor' };
  }

  // Tier 1: Report generation
  if (task.phase === 'report') {
    return { tier: 1, model: 'executor' };
  }

  // Tier 4: Batch scraping
  if (task.pageCount > 50) {
    return { tier: 4, model: 'batch_gpu' };
  }

  // Default: Tier 1
  return { tier: 1, model: 'executor' };
}
```

**RouteLLM Integration**:
- Complexity scoring for ambiguous tasks
- Logged routing decisions for audit
- Manual override via workflow parameter

**Validation**:
- ✅ Are 40%+ of tasks routed to Tier 0?
- ✅ Is planning always Tier 3?
- ✅ Is execution mostly Tier 1?
- ✅ Does batch detection work (>50 operations → Tier 4)?

---

### VII. Tool Integration Standards

**Requirement**: External tools MUST follow structured integration patterns with proper error handling.

**Rationale**: Agents interact with many external tools (git, Playwright, Firecrawl). Standardized integration prevents fragile workflows.

**Integration Patterns**:

**1. File System Operations** (Coder Agent)
```javascript
// n8n Code Node: Read File
const fs = require('fs');
const path = $json.filePath;

try {
  if (!fs.existsSync(path)) {
    throw new Error(`File not found: ${path}`);
  }

  const content = fs.readFileSync(path, 'utf-8');
  return [{
    filePath: path,
    content: content,
    lines: content.split('\n').length,
    size: Buffer.byteLength(content, 'utf-8')
  }];
} catch (error) {
  return [{
    error: true,
    message: error.message,
    filePath: path
  }];
}
```

**2. Git Operations** (Coder Agent)
```javascript
// n8n Code Node: Git Commit
const { execSync } = require('child_process');
const codebasePath = $json.codebasePath;
const message = $json.commitMessage;
const files = $json.modifiedFiles;

try {
  // Change to codebase directory
  process.chdir(codebasePath);

  // Add files
  files.forEach(file => {
    execSync(`git add ${file}`);
  });

  // Commit
  const commitHash = execSync(`git commit -m "${message}" --author="AI Agent <agent@n8n>"`).toString().trim();

  // Push (optional)
  if ($json.autoPush) {
    const branch = execSync('git branch --show-current').toString().trim();
    execSync(`git push origin ${branch}`);
  }

  return [{
    success: true,
    commitHash: commitHash,
    branch: execSync('git branch --show-current').toString().trim(),
    filesCommitted: files.length
  }];
} catch (error) {
  return [{
    error: true,
    message: error.message,
    stderr: error.stderr?.toString()
  }];
}
```

**3. Browser Automation** (Web Agent)
```javascript
// n8n HTTP Request Node: Call Playwright Service
// URL: http://playwright-service:3000/navigate
// Method: POST
// Body:
{
  "action": "navigate",
  "url": "https://example.com",
  "screenshot": true,
  "waitFor": "networkidle"
}

// Response:
{
  "success": true,
  "url": "https://example.com",
  "screenshot": "/screenshots/page_12345.png",
  "html": "<html>...",
  "duration_ms": 2100
}
```

**4. Web Scraping** (Web Agent)
```javascript
// n8n HTTP Request Node: Call Firecrawl
// URL: https://api.firecrawl.dev/v0/scrape
// Method: POST
// Headers: Authorization: Bearer ${FIRECRAWL_API_KEY}
// Body:
{
  "url": "https://example.com/pricing",
  "formats": ["markdown", "structured"],
  "onlyMainContent": true
}

// Response:
{
  "success": true,
  "markdown": "# Pricing\n\n| Plan | Price |\n|------|-------|\n...",
  "structured": {
    "pricing_tiers": [
      { "name": "Basic", "price": "$10/mo" },
      { "name": "Pro", "price": "$50/mo" }
    ]
  }
}
```

**Tool Registry**:

| Tool | Use Case | Agent | Integration Method |
|------|----------|-------|-------------------|
| **File I/O** | Read/write code | Coder | n8n Code Node (fs module) |
| **Git** | Commit automation | Coder | n8n Code Node (child_process) |
| **Playwright** | Browser automation | Web | HTTP Request → Playwright service |
| **Firecrawl** | Web scraping | Web | HTTP Request → Firecrawl API |
| **Tree-sitter** | AST parsing | Coder | HTTP Request → Parser service |
| **Qdrant** | Vector search | Both | HTTP Request → Qdrant API |
| **LiteLLM** | LLM calls | Both | HTTP Request → LiteLLM API |

**Error Handling Standard**:
- ALL tool calls MUST have try/catch
- ALL tool calls MUST have timeout (30s default)
- ALL errors MUST return structured error object:
  ```json
  {
    "error": true,
    "tool": "playwright",
    "message": "Navigation timeout",
    "context": { "url": "...", "timeout_ms": 30000 }
  }
  ```

**Validation**:
- ✅ Do all file operations validate paths before reading?
- ✅ Do all git operations check for repo existence?
- ✅ Do all browser operations have timeout and cleanup?
- ✅ Do all tool errors return structured error objects?

---

### VIII. Cost-Optimized Provider Selection

**Requirement**: Provider orchestration MUST consider both model capability AND pricing model economics.

**Rationale**: Different providers have different pricing models (hourly vs per-token). Optimal provider depends on workload volume.

**Pricing Models**:

| Provider Category | Pricing | Best For | Providers |
|-------------------|---------|----------|-----------|
| **Local (Free)** | $0 | All simple tasks, vision | Ollama (Tier 0/2) |
| **Per-Token** | $0.20-0.80/M | Bursty traffic, <50 ops | Fireworks, Together (Tier 1) |
| **Premium APIs** | $3-15/M | Complex reasoning only | Claude, Gemini (Tier 3) |
| **Hourly VRAM** | $0.69-2/hr | Batch ops >50, sustained load | RunPod, Salad (Tier 4) |

**Dynamic Provider Selection**:

```javascript
function selectProvider(task, batchSize) {
  // Local first (free)
  if (task.complexity <= 0.2 || task.tier === 0) {
    return { provider: 'ollama', model: 'llama3.2:3b', cost: 0 };
  }

  // Vision tasks → local multimodal
  if (task.hasImages && task.tier === 2) {
    return { provider: 'ollama', model: 'llama3.2-vision:70b', cost: 0 };
  }

  // Batch detection for Tier 4
  if (batchSize >= 50) {
    const tier1Cost = batchSize * 0.0003;  // Per-token cost
    const tier4Cost = (batchSize * 5 / 3600) * 0.69;  // Hourly cost

    if (tier4Cost < tier1Cost * 0.7) {  // 30% savings threshold
      return { provider: 'runpod', model: 'llama-405b', cost: tier4Cost, batch: true };
    }
  }

  // Standard execution → per-token
  if (task.tier === 1) {
    return { provider: 'fireworks', model: 'llama-8b', cost: 0.0003 };
  }

  // Planning/review → premium
  if (task.tier === 3) {
    return { provider: 'anthropic', model: 'claude-3.5-sonnet', cost: 0.012 };
  }

  // Fallback
  return { provider: 'fireworks', model: 'llama-8b', cost: 0.0003 };
}
```

**Budget Pool Management**:
- `hourly_vram`: Tier 4 GPU rental ($100/month)
- `per_token`: Tier 1 providers ($50/month)
- `premium`: Tier 3 planning/review ($50/month)
- `local_vps`: Tier 0/2 infrastructure ($75/month)

**Auto-Shutdown**:
- Tier 4 instances auto-shutdown after 30 min idle
- Prevents wasting hourly charges
- n8n scheduled workflow checks instance uptime every 15 min

**Validation**:
- ✅ Does system calculate per-token vs hourly cost before routing?
- ✅ Do Tier 4 instances shutdown automatically when idle?
- ✅ Are budget pools tracked separately?

---

### IX. Architect/Executor Pattern

**Requirement**: Multi-step tasks MUST use tiered agent roles for cost optimization.

**Rationale**: Tier 3 models excel at planning but are expensive. Tier 0/1 models can execute simple subtasks cheaply. Separating roles achieves 50%+ cost savings.

**Pattern**:
1. **Architect** (Tier 3): Analyze task → Create detailed plan with subtasks
2. **Executor** (Tier 0/1): Execute each subtask independently
3. **Reviewer** (Tier 3): Validate results → Provide feedback

**Economics**:
```
Traditional (all Tier 3):
- Planning: $0.15
- Execute 10 steps × $0.15 = $1.50
- Review: $0.15
- Total: $1.80

Architect/Executor:
- Architect (Tier 3): $0.15
- Execute 10 steps × $0.035 (Tier 1) = $0.35
- Reviewer (Tier 3): $0.15
- Total: $0.65
- Savings: 64%
```

**Coder Agent Implementation**:
```javascript
// Phase 1: Architect (Tier 3)
const plan = await callLLM({
  model: 'architect',
  prompt: `Task: ${userTask}\nCodebase: ${codebaseStructure}\n\nCreate a detailed plan with 10-15 executable subtasks.`
});

// Phase 2: Execute (Tier 0/1)
const results = [];
for (const subtask of plan.subtasks) {
  const complexity = analyzeComplexity(subtask);
  const tier = complexity < 0.3 ? 0 : 1;

  const result = await callLLM({
    model: tier === 0 ? 'fast_filter' : 'executor',
    prompt: subtask.description
  });

  results.push(result);
}

// Phase 3: Review (Tier 3)
const review = await callLLM({
  model: 'architect',
  prompt: `Review the following implementation:\n${JSON.stringify(results)}\n\nValidate quality and suggest improvements.`
});
```

**Web Agent Implementation**:
```javascript
// Phase 1: Architect (Tier 3)
const plan = await callLLM({
  model: 'architect',
  prompt: `Task: ${webTask}\nTarget: ${targetURL}\n\nCreate a step-by-step web automation plan.`
});

// Phase 2: Execute (Tier 1/2)
const results = [];
for (const step of plan.steps) {
  if (step.requiresVision) {
    // Use Tier 2 for screenshot analysis
    const result = await callLLM({
      model: 'vision',
      prompt: step.description,
      image: step.screenshot
    });
  } else {
    // Use Tier 1 for text processing
    const result = await callLLM({
      model: 'executor',
      prompt: step.description
    });
  }
  results.push(result);
}

// Phase 3: Generate Report (Tier 1)
const report = await callLLM({
  model: 'executor',
  prompt: `Generate a summary report from these results:\n${JSON.stringify(results)}`
});
```

**When to Use**:
- ✅ Multi-step coding tasks (refactoring, feature implementation)
- ✅ Multi-page web scraping (research, competitor analysis)
- ✅ Batch operations (process >10 files/pages)
- ❌ Single-step tasks (simple queries, one file modification)

**Validation**:
- ✅ Are multi-step tasks using architect/executor?
- ✅ Is cost breakdown logged per role?
- ✅ Are savings vs all-Tier-3 measured?

---

### X. Agent Specialization

**Requirement**: Each agent type MUST have specialized capabilities for its domain while sharing the same cost optimization framework.

**Rationale**: Coder and Web agents have fundamentally different tool requirements. Specialization improves quality while maintaining architectural consistency.

**Coder Agent Specialization**:

**Capabilities**:
- ✅ Read/write files in codebase
- ✅ Parse code structure (AST via tree-sitter)
- ✅ Search codebase via RAG (vector search)
- ✅ Execute git operations (branch, commit, push)
- ✅ Run linters/formatters (optional)
- ✅ Generate code (functions, classes, tests)

**Tools**:
- File System: n8n Code Node (fs module)
- Git: n8n Code Node (child_process)
- AST Parser: HTTP Request → tree-sitter service
- RAG: HTTP Request → Qdrant
- Linters: HTTP Request → eslint/pylint service (optional)

**Constraints**:
- MUST validate codebase path exists before operations
- MUST create git branch before modifications
- MUST commit changes after modifications
- MUST never delete files without explicit user confirmation
- MUST never modify files outside specified codebase path

**Example Tasks**:
- "Refactor authentication to use JWT tokens"
- "Add logging to all API endpoints"
- "Find and fix all ESLint errors"
- "Write unit tests for user service"
- "Extract this function into a reusable utility"

---

**Web Agent Specialization**:

**Capabilities**:
- ✅ Navigate websites (Playwright)
- ✅ Extract structured data (Firecrawl)
- ✅ Analyze screenshots (Vision model)
- ✅ Fill forms and click buttons (Playwright)
- ✅ Handle authentication (cookies, sessions)
- ✅ Respect rate limits and robots.txt

**Tools**:
- Browser Automation: HTTP Request → Playwright service
- Web Scraping: HTTP Request → Firecrawl API
- Vision Analysis: HTTP Request → LiteLLM (Tier 2)
- Content Storage: HTTP Request → Qdrant (for future RAG)

**Constraints**:
- MUST run browsers in headless mode
- MUST respect robots.txt
- MUST add delays between requests (rate limiting)
- MUST close browser sessions on completion
- MUST save screenshots on errors for debugging
- MUST never log credentials or sensitive data

**Example Tasks**:
- "Research competitor pricing from their website"
- "Monitor this page for price changes daily"
- "Extract all product reviews from Amazon"
- "Fill out this contact form with test data"
- "Login to dashboard and download monthly reports"

---

**Shared Framework** (Both Agents):

Both agents share the same:
- ✅ 5-tier cost optimization
- ✅ Pre-budget checks
- ✅ Architect/executor pattern
- ✅ Observability (logging, tracing, cost tracking)
- ✅ Error handling (rollback, cleanup)
- ✅ LiteLLM integration

**Agent Selection Logic**:
```javascript
function selectAgent(userInput) {
  // Detect codebase path
  const hasCodebasePath = /\/[a-z0-9_\-\/]+/i.test(userInput);

  // Detect code-related keywords
  const codeKeywords = ['refactor', 'function', 'class', 'file', 'code', 'git', 'commit', 'test'];
  const hasCodeKeyword = codeKeywords.some(kw => userInput.toLowerCase().includes(kw));

  // Detect web-related keywords
  const webKeywords = ['website', 'scrape', 'navigate', 'extract', 'url', 'browser', 'page'];
  const hasWebKeyword = webKeywords.some(kw => userInput.toLowerCase().includes(kw));

  // Detect URL
  const hasURL = /https?:\/\/[^\s]+/.test(userInput);

  // Select agent
  if (hasCodebasePath || hasCodeKeyword) {
    return 'coder';
  } else if (hasURL || hasWebKeyword) {
    return 'web';
  } else {
    // Ambiguous - ask user or use Tier 0 to classify
    return classifyIntent(userInput);
  }
}
```

**Validation**:
- ✅ Does Coder Agent only operate within specified codebase?
- ✅ Does Web Agent always close browser sessions?
- ✅ Can both agents use architect/executor pattern?
- ✅ Are agent-specific tools properly isolated?

---

## Architecture Constraints

### State Management
- Explicit state passing: JSON objects between n8n nodes
- No hidden globals: All state in workflow execution context
- Agent state includes: task, codebase/url, phase, tier, cost, results
- Persistence: Long-running workflows save state to n8n database

### Vector Database Standards (Qdrant)
- **Coder Agent Collections**: One collection per codebase (`codebase_{name}`)
  - Vectors: Code chunk embeddings (768 dimensions)
  - Metadata: file_path, function_name, class_name, line_start, line_end

- **Web Agent Collections**: One collection for web content (`web_content`)
  - Vectors: Web page chunk embeddings (768 dimensions)
  - Metadata: url, page_title, extraction_date, content_type

- Backup: n8n scheduled workflow calls Qdrant snapshot API daily

### LLM Interface & Routing Layer
- **Unified API**: All LLM calls through LiteLLM HTTP wrapper (OpenAI-compatible)
- **Intelligent Routing**: RouteLLM determines tier based on complexity
- **Cost Orchestration**: Pre-budget check validates pool balance before LLM call (MANDATORY)
- **Cost Proxy**: Helicone sits between LiteLLM and providers (caching, logging)
- **Request Flow**: n8n → Pre-Budget → RouteLLM → LiteLLM → Helicone → Provider
- **Configuration**: Model groups, routing thresholds, fallback chains in litellm-config.yaml
- **Override**: Manual tier selection via workflow parameter

### Git Standards (Coder Agent Only)
- **Branch naming**: `ai-agent/{task-id}` (auto-generated)
- **Commit message format**:
  ```
  <type>: <short summary>

  <detailed description>

  - Change 1
  - Change 2

  Co-authored-by: AI Agent <agent@n8n>
  ```
- **Types**: feat, fix, refactor, test, docs, chore
- **Auto-push**: Optional (default: false, user must manually review and push)

### Browser Automation Standards (Web Agent Only)
- **Playwright version**: Latest stable
- **Browser**: Chromium (headless)
- **Timeout**: 30s per action
- **Screenshots**: Saved on every page navigation and on errors
- **Cleanup**: Always close browser in try/finally block

---

## Quality Standards

### Testing Requirements
- **Coder Agent**: Test with sample codebases (10+ files, different languages)
- **Web Agent**: Test with sample websites (public pages, no auth required initially)
- **Cost Validation**: Compare actual cost vs estimated cost (< 10% variance)
- **Tier Routing**: Validate tier selection accuracy (>90% correct tier)
- **Error Handling**: Test rollback on Coder errors, browser cleanup on Web errors

### Performance Benchmarks
- **Response time**: <10s for simple tasks (Tier 0/1), <30s for complex (Tier 3)
- **Codebase indexing**: >100 files/minute
- **Web scraping**: >10 pages/minute
- **Concurrent support**: 5+ simultaneous agent tasks

### Security
- **API keys**: Stored in n8n Credentials (encrypted at rest)
- **Git operations**: Never commit API keys or credentials
- **Web agent**: Never log passwords or session tokens
- **File operations**: Validate paths prevent directory traversal
- **Sandboxing**: Coder Agent cannot access files outside codebase path

---

## Expected Cost Savings (v4.0)

**Baseline (All Tier 3)**:
- 100 agent tasks/month × $0.50/task = $50/month

**v4.0 (Intelligent Routing)**:
- Tier 0 (40%): 40 tasks × $0 = $0
- Tier 1 (35%): 35 tasks × $0.10 = $3.50
- Tier 2 (5%): 5 tasks × $0 = $0
- Tier 3 (18%): 18 tasks × $0.50 = $9.00
- Tier 4 (2%): 2 tasks × $0.30 = $0.60
- **Total: $13.10**

**With Optimizations**:
- Prompt compression: Save $2
- Architect/executor: Save $4
- Helicone cache: Save $1
- **Final: ~$6-8/month (84-88% savings)**

---

## Document Version
**Constitution Version**: 4.0.0
**Last Updated**: 2025-11-18
**Breaking Changes**: Yes - Complete pivot to multi-agent orchestrator
**Migration**: See ARCHITECTURE_v4.0_COMPLETE.md for migration guide
**Status**: Ready for implementation
