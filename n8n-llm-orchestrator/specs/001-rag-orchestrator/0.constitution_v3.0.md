<!--
Sync Impact Report
==================
Version: 2.1.0 → 3.0.0 (LiteLLM Migration + 5-Tier System + Architect Pattern)
Modified Principles: II (LiteLLM replaces aisuite), IV (Pre-budget checks), VIII (5-tier routing)
Added Sections: IX (Architect/Executor Pattern), 5-Tier Model Matrix, Batch Routing Logic
Breaking Changes:
  - Replace aisuite with LiteLLM (production-ready, better support)
  - Add Tier 0 (fast filter) and split Tier 2 into local multimodal
  - Pre-budget checks before all LLM calls (not just post-logging)
  - Batch size routing (>50 requests → GPU rental)
  - Prompt compression for expensive tiers
  - MCP Pointer for browser automation (replaces generic Browser-Use)
Templates Status: ✅ Requires update to all specs, plan, research, tasks
Follow-up TODOs:
  - Replace aisuite with LiteLLM in all workflows
  - Add Tier 0 classifier nodes
  - Implement pre-budget check Code nodes
  - Add batch detection logic
  - Configure MCP Pointer for web agent
  - Update provider pricing matrix
-->

# n8n Multi-Modal Agent Orchestrator Constitution v3.0

**Capabilities**: Document RAG, Web Agent, Coder Agent, Multi-Tier Cost Orchestration
**Ratified**: 2025-10-30
**Last Amended**: 2025-11-18

---

## Core Principles

### I. Visual-First Design
All agentic workflows MUST be representable in n8n's visual interface. This ensures:
- Non-technical stakeholders can understand agent flows at a glance
- Debugging is visual and intuitive
- No hidden logic in external scripts unless absolutely necessary for performance
- Complex multi-agent orchestrations remain maintainable
- **Web agent tasks**: Browser automation steps visible as n8n nodes
- **Coder agent tasks**: Architect/executor pattern visible in workflow

**Rationale**: Visual workflows reduce cognitive load and make agentic systems accessible to broader teams.

### II. LLM-Agnostic & Modality-Aware Architecture
The system MUST support multiple LLM providers interchangeably with modality awareness:

**Supported Providers** (via **LiteLLM**):
- OpenAI (GPT-4, GPT-4 Turbo, GPT-4o) - Text + Vision
- Anthropic (Claude 3.5 Sonnet, Claude 3 Opus) - Text + Vision
- Google (Gemini 1.5 Pro, Gemini 2.0 Flash) - Text + Vision + Multimodal
- Ollama (local models: Llama 3.1 8B, 70B, Llama 3.2 Vision) - Text + Vision
- Fireworks.ai (Llama 3.1 70B, Mixtral) - Text-only
- Together.ai (Llama 3.2 Vision, Qwen2.5 72B, DeepSeek-V2.5) - Text + Vision
- RunPod (Llama 3.1 405B, custom models) - Text + Vision (hourly VRAM)
- Salad (Llama 3.1 70B) - Text-only (hourly VRAM)
- CloudRift (custom models) - Text + Vision (hourly VRAM)

**Requirements**:
- ALL LLM calls MUST go through unified interface layer (**LiteLLM**, not aisuite)
- Provider switching MUST NOT require workflow redesign
- Each agent can specify preferred provider AND modality requirements (text/vision/multimodal)
- Fallback chains supported (e.g., Fireworks → Ollama → Claude → GPT-4)
- n8n workflows interact with LiteLLM HTTP API (OpenAI-compatible)
- **Modality detection**: Automatically select vision-capable models for screenshot analysis tasks
- **Provider aliasing**: Use model groups (architect, executor, vision) not specific models

**Rationale**: LiteLLM is production-proven, widely adopted, better documented than aisuite. Avoids vendor lock-in, enables cost optimization across pricing models. Single API interface reduces n8n complexity.

### III. Fail-Safe Operations (NON-NEGOTIABLE)
Every LLM call, tool execution, data retrieval, and **browser action** MUST implement error handling:
- Automatic retries with exponential backoff (max 3 attempts)
- Graceful degradation when services unavailable
- Meaningful error messages returned to user
- Circuit breaker pattern for repeated failures
- No workflow crash—always return actionable response
- **Web agent**: Browser failures trigger retry with alternative strategy or human escalation
- **Provider failures**: Tier fallback chain (e.g., Tier 1 fails → Tier 2 local → Tier 3 premium)
- **Budget exhaustion**: Downgrade to lower tiers automatically

**Rationale**: LLM APIs and browser automation are inherently unreliable; production systems require resilience.

### IV. Cost-Conscious Design with Pre-Budget Checks
Token usage and API costs MUST be actively managed with **proactive budget control**:

**Pre-Flight Budget Checks** (NEW):
- **BEFORE** every LLM call: Check remaining budget vs estimated cost
- If estimated cost > remaining budget: Downgrade tier automatically
- Alert at 80% budget utilization
- Block requests at 100% (emergency mode)

**Cost Tracking**:
- Monitor and log token consumption per workflow via **Helicone** proxy
- Track real-time costs with **MCPcat** production monitoring
- Implement prompt optimization strategies (**prompt compression** via Tier 0 for Tier 3)
- Cache LLM responses via **Helicone** (1-hour TTL)
- Prefer local Ollama for development and high-volume use cases
- Use **RouteLLM** to route simple queries to cheaper models
- Track monthly spend with budget alerts via **LangSmith** or **Langfuse**

**Advanced Optimization**:
- **Batch size routing**: >50 requests → GPU rental (Tier 4) amortizes hourly cost
- **Workload analysis**: Queries/hour tracking determines hourly vs per-token provider
- **Prompt compression**: Use Tier 0 (Ollama 3B) to compress prompts before Tier 3 calls (60% token reduction)
- **Intelligent caching**: Semantic similarity caching (not just exact match)
- **Budget pools**: Separate pools for hourly, per-token, premium APIs

**Rationale**: Proactive budget checks prevent overruns (reactive logging is too late). Different pricing models require different strategies. Batch detection and prompt compression are critical optimizations.

### V. Observable Agent Flows
Every agent step MUST be traceable and debuggable:
- Distributed tracing with **OpenTelemetry** across n8n → LiteLLM → providers
- Structured logging with context (agent name, step, timestamp, tier, cost) via **MCPcat**
- Capture all LLM prompts and responses in **LangSmith** or **Langfuse**
- Track RAG retrieval results (chunks retrieved, relevance scores)
- Execution metrics (latency, token count, success rate, tier used) via **Helicone**
- Full request traces with visualization in **LangSmith** dashboard
- n8n execution logs retained for 30 days minimum
- **NEW**: Log all browser actions (navigate, click, type, screenshot) to audit trail
- **NEW**: Capture screenshots at each web agent step for visual debugging
- **NEW**: Log routing rationale (complexity, modality, volume, cost, tier selected)

**Rationale**: Complex multi-agent systems are opaque without comprehensive observability. Visual debugging via screenshots is critical for web agents. Routing decision auditability enables optimization.

### VI. Intelligent Routing
Query complexity MUST determine model selection automatically:
- **RouteLLM** analyzes queries and routes to appropriate model tier
- **Tier 0** (Ollama 3B-13B): Fast classification, simple filters (<500ms)
- **Tier 1** (Fireworks/Together 70B): Standard tasks, medium reasoning ($0.20-0.80/M)
- **Tier 2** (Ollama 70B + Vision): Local multimodal, screenshot analysis (VPS fixed cost)
- **Tier 3** (Claude/Gemini): Complex reasoning, architecture planning ($3-15/M)
- **Tier 4** (RunPod/Salad GPU): Batch processing >50 requests ($0.69-2/hour)
- Routing decisions logged and auditable
- Manual override available per workflow via configuration
- **NEW**: Modality-aware routing (text-only vs vision vs multimodal)
- **NEW**: Batch size detection (triggers Tier 4 for cost optimization)

**Rationale**: 5-tier system provides finer granularity than 4-tier. Tier 0 fast filter reduces overall costs. Batch detection critical for high-volume scenarios.

### VII. Tool Integration Standards
External tools and data sources MUST follow MCP (Model Context Protocol) patterns:
- **Web scraping (read-only)**: Use **Firecrawl** for LLM-friendly structured output
- **Web automation (read-write)**: Use **MCP Pointer** for browser DOM interaction
- **Documentation access**: Use **PageIndex MCP** for vectorless RAG (cost-efficient)
- **Browser infrastructure**: Use **Browserbase** for headless browser hosting
- **Additional extraction**: Use **Jina AI Reader** for clean text extraction
- **Code integration** (future): Use **Aider** for git operations
- **App integrations** (future): Use **Composio** or **Rube** for standardized API access
- **Authentication** (future): Use **AgentAuth** for OAuth flows and token management
- All tools MUST expose consistent interfaces via n8n HTTP Request nodes
- Tool failures MUST NOT crash workflows (fallback to degraded functionality)

**Rationale**: Standardized tool integration reduces maintenance burden. MCP Pointer is production-tested for browser automation. Multiple extraction tools provide fallback options.

### VIII. Cost-Optimized Provider Selection

Provider orchestration MUST consider both model capability AND pricing model economics:

#### 5-Tier Model Matrix

**Tier 0: Local Fast Filter** (Ollama 7B-13B)
- **Models**: Llama 3.2 3B, Phi-3.5, Gemma 2 9B
- **Cost**: $0/token (local)
- **Latency**: <500ms
- **Use Cases**: Request classification, intent detection, prompt compression
- **Pricing Model**: Free (VPS included)

**Tier 1: Hosted Open Source** (Fireworks/Together)
- **Models**: Llama 3.1 70B, Qwen2.5 72B, DeepSeek-V2.5
- **Cost**: $0.20-$0.80 per 1M tokens
- **Latency**: 1-2s
- **Use Cases**: Standard coding, text generation, API responses
- **Pricing Model**: Per-token (bursty traffic)

**Tier 2: Local Multimodal** (Ollama 70B + Vision)
- **Models**: Llama 3.1 70B, Llama 3.2 Vision 90B, Qwen2-VL 7B
- **Cost**: $0/token (VPS fixed ~$75/month)
- **Latency**: 2-5s
- **Use Cases**: Image analysis, screenshot understanding, PDF extraction
- **Pricing Model**: Free (unlimited once VPS paid)

**Tier 3: Premium APIs** (Claude/Gemini)
- **Models**: Claude 3.5 Sonnet, Gemini 2.0 Flash
- **Cost**: $3-$15 per 1M tokens
- **Latency**: 1-3s
- **Use Cases**: Architecture planning, code review, complex reasoning
- **Pricing Model**: Per-token (high-value queries only)

**Tier 4: GPU Rental** (RunPod/Salad/CloudRift)
- **Models**: Llama 3.1 405B, custom fine-tuned
- **Cost**: $0.69-$2 per hour (fixed)
- **Latency**: Variable (30-90s spin-up)
- **Use Cases**: Batch processing (>50 requests), long sessions
- **Pricing Model**: Hourly (amortize over many queries)

#### Routing Decision Logic

```javascript
// Enhanced routing with 5 tiers + batch detection
function selectTier(input) {
  const { prompt, hasImages, complexity, batchSize, budgetRemaining } = input;

  // Tier 0: Fast filter (always try first for text-only)
  if (!hasImages && (complexity <= 2 || prompt.length < 500)) {
    return { tier: 0, model: 'ollama-3b', cost: 0 };
  }

  // Tier 2: Multimodal required
  if (hasImages) {
    return { tier: 2, model: 'ollama-vision-90b', cost: 0 };
  }

  // Tier 4: Batch processing economics
  if (batchSize >= 50) {
    const tier1Cost = batchSize * 0.0003; // ~$0.30 per 1K requests
    const tier4Cost = (batchSize * 5) / 60 * 0.69; // 5s per req @ $0.69/hour
    if (tier4Cost < tier1Cost * 0.7) { // 30% savings threshold
      return { tier: 4, model: 'llama-405b', provider: 'runpod', cost: tier4Cost, spinUp: true };
    }
  }

  // Tier 1: Standard tasks (if budget allows)
  if (complexity <= 6 && budgetRemaining > 5) {
    return { tier: 1, model: 'llama-70b', provider: 'fireworks', cost: 0.0002 };
  }

  // Tier 3: Complex/critical (if budget allows)
  if (complexity >= 7 && budgetRemaining > 2) {
    return { tier: 3, model: 'claude-sonnet-4', cost: 0.003 };
  }

  // Default: Tier 2 local (free)
  return { tier: 2, model: 'ollama-70b', cost: 0 };
}
```

#### Budget Pool Management

**Monthly Budget Allocation**:
- **Pool A (Hourly VRAM)**: $100/month → RunPod, Salad, CloudRift
- **Pool B (Per-Token Open)**: $50/month → Fireworks, Together
- **Pool C (Premium APIs)**: $30/month → Claude, Gemini
- **Pool D (Local VPS)**: $75/month (fixed) → Unlimited Tier 0 & Tier 2

**Pre-Flight Check**:
```javascript
function checkBudget(route, dailyBudget) {
  const spent = getSpentToday();
  const remaining = dailyBudget - spent;

  if (route.cost > remaining) {
    // Downgrade strategy
    if (route.tier === 3) return selectTier({...input, forceMaxTier: 2});
    if (route.tier === 1) return selectTier({...input, forceMaxTier: 2});
    if (route.tier === 4) return selectTier({...input, forceMaxTier: 1});
    throw new Error(`Budget exceeded: $${spent}/$${dailyBudget}`);
  }

  if (spent / dailyBudget > 0.8) {
    sendAlert('Budget at 80%', { spent, remaining });
  }

  return { approved: true, remaining };
}
```

**Rationale**: 5 tiers provide optimal granularity. Tier 0 fast filter is critical cost saver. Batch detection prevents overpaying for high-volume workloads. Pre-budget checks are MANDATORY to prevent overruns.

### IX. Architect/Executor Pattern (NEW)

For complex multi-step tasks (Coder Agent), separate planning from execution:

**Architect Role** (Tier 3: Claude 3.5 Sonnet):
- Analyzes user request and creates detailed plan
- Breaks down into discrete, executable subtasks
- Estimates complexity per subtask (1-10 scale)
- Defines success criteria for each step
- **Cost**: Premium tier justified by quality of planning

**Executor Role** (Tier 0/1/2: Context-dependent):
- Implements individual subtasks from architect plan
- Routes each subtask based on its specific complexity
- Simple subtasks → Tier 0 or Tier 1
- Complex subtasks → Tier 2 or escalate to Tier 3
- **Cost**: Majority of work done on cheaper tiers

**Review Role** (Tier 3: Claude 3.5 Sonnet):
- Final validation of combined output
- Ensures consistency and correctness
- Provides feedback for iteration if needed
- **Cost**: Small premium cost for quality assurance

**Economics**:
```
Traditional (all Tier 3): 10 steps × $0.15 = $1.50
Architect Pattern:
  - Planning (Tier 3): $0.15
  - Execution (7×Tier 1 + 2×Tier 2 + 1×Tier 3): $0.35
  - Review (Tier 3): $0.15
  Total: $0.65 (57% savings)
```

**Rationale**: Architect/executor pattern optimizes cost-quality tradeoff. Planning benefits from premium models, but execution often doesn't. Enables parallelization of executor tasks.

---

## Architecture Constraints

### Vector Database Standards
- **Primary**: Qdrant (self-hosted) for production RAG workflows with embeddings
- **Alternative (Cost-Optimized)**: PageIndex MCP for vectorless RAG on documentation
- **Alternative (Cloud)**: Supabase Vector for cloud-hosted scenarios
- All embeddings MUST use consistent model (nomic-embed-text, 768 dimensions)
- Collections MUST include metadata: source, timestamp, chunk_id, version
- Backup collections daily via Qdrant snapshot API
- Use PageIndex MCP for frequently-accessed documentation to reduce embedding costs

### Document Processing Rules
- Chunk size: 800 characters (±200 acceptable for specific use cases)
- Chunk overlap: 200 characters minimum
- Top-k retrieval: 3-5 chunks (never exceed 10 to prevent context overflow)
- Re-ranking optional but recommended for precision-critical applications

### State Management
- Agent state MUST be passed explicitly between n8n nodes (no hidden globals)
- Long-running workflows MUST persist state to prevent data loss
- Maximum workflow execution time: 5 minutes (configurable per workflow)
- **Web agent**: Browser state (cookies, session) persisted between steps
- **Coder agent**: Task breakdown state passed through executor loop

### LLM Interface & Routing Layer (Updated)
- **Unified API**: All LLM calls routed through **LiteLLM** HTTP wrapper service (OpenAI-compatible)
- **Intelligent Routing**: **RouteLLM** + complexity estimation determines optimal tier
- **Batch Detection**: Code node analyzes queue depth → triggers Tier 4 if cost-effective
- **Cost Proxy**: **Helicone** sits between LiteLLM and providers for caching/logging
- **Request Flow**: n8n → Budget Pre-Check → RouteLLM (tier selection) → LiteLLM → Helicone → Provider
- **Configuration**: Model groups aliased in LiteLLM config (architect, executor-cloud, executor-local, vision)
- **Override**: Manual tier/model selection available via workflow parameters

### Web Agent Standards
- **Browser Automation**: **MCP Pointer** (preferred) or Playwright MCP for browser control
- **Supported Actions**: navigate, click, type, select, submit, screenshot, scroll, wait
- **State Management**: Cookies, localStorage, sessionStorage persisted between steps
- **Error Handling**: Retry with exponential backoff, alternative selectors, human escalation
- **Visual Understanding**: Screenshot analysis via Tier 2 vision (Ollama) or Tier 3 (Claude 3.5 Vision)
- **Multi-Step Tasks**: Task decomposition, state persistence, resume on failure
- **Security**: No CAPTCHA bypass (human escalation), respect robots.txt, rate limiting
- **Audit Trail**: All actions logged with screenshots to MCPcat + LangSmith

### Observability Stack (Required for Production)
- **Distributed Tracing**: OpenTelemetry with Jaeger/Zipkin backend
- **LLM Monitoring**: LangSmith (preferred) or Langfuse (opensource alternative)
- **Cost Tracking**: Helicone proxy with real-time dashboard + pre-flight budget checks
- **Agent Metrics**: MCPcat for tool call tracking, tier distribution, error monitoring
- **Integration**: All tools export traces to OpenTelemetry collector
- **Retention**: 30 days for execution logs, 90 days for aggregated metrics
- **Alerting**: Budget alerts (80%, 90%, 100%), error rate thresholds, latency spikes, idle instance warnings

---

## Quality Standards

### Testing Requirements
- All RAG pipelines MUST have evaluation datasets with ground truth
- Measure and track: retrieval accuracy (recall@k), answer relevance, latency
- Integration tests required for: document ingestion, query workflows, fallback paths, **web agent tasks**, **coder agent tasks**
- Manual testing protocol documented for each workflow before production
- **Web agent**: Test on 10+ diverse websites (e-commerce, news, documentation, forms)
- **Cost routing**: Simulate high-volume and low-volume workloads, verify correct tier selection
- **Budget enforcement**: Test pre-flight checks reject requests when budget exceeded

### Performance Benchmarks
- RAG query response time: < 5 seconds end-to-end
- Document ingestion: > 10 docs/minute
- Concurrent workflow support: minimum 5 simultaneous executions
- LLM response timeout: 30 seconds (configurable)
- **Web agent**: < 30 seconds for simple tasks (<5 steps), < 2 minutes for complex tasks
- **Tier 0 latency**: < 500ms (fast filter classification)
- **Tier 4 spin-up**: < 90 seconds (GPU instance startup)
- **Budget check overhead**: < 100ms (negligible)

### Security
- API keys stored in n8n credentials (never hardcoded)
- Vector database access restricted to local network or VPN
- User input sanitized before LLM prompts
- No PII in logs unless explicitly required and compliant
- **Web agent**: No CAPTCHA bypass, respect robots.txt, rate limiting on target sites
- **Provider security**: Separate credentials per provider, least-privilege access
- **LiteLLM security**: API key rotation, request signing, rate limiting

---

## Governance

This constitution supersedes all other development practices. Changes require:
1. Proposal with rationale and impact analysis
2. Version bump following semantic versioning (MAJOR.MINOR.PATCH)
3. Update to all dependent templates and documentation
4. Team review (or author review for solo projects)

All workflows and implementations MUST be verified against these principles during:
- Design review (before implementation)
- Code/workflow review (before deployment)
- Post-deployment audit (after 1 week in production)

Complexity that violates these principles MUST be explicitly justified and documented as technical debt with remediation plan.

**Version**: 3.0.0 | **Ratified**: 2025-10-30 | **Last Amended**: 2025-11-18

---

## Summary of v3.0 Changes

**Major Breaking Changes**:
1. **LiteLLM replaces aisuite**: Production-ready, better support, wider adoption
2. **5-tier system**: Added Tier 0 (fast filter), split Tier 2 (local multimodal)
3. **Pre-budget checks**: MANDATORY before all LLM calls (not just post-logging)
4. **Architect/Executor pattern**: Separate planning (Tier 3) from execution (Tier 0/1/2)
5. **Batch size routing**: Automatic detection → Tier 4 GPU rental for >50 requests
6. **Prompt compression**: Use Tier 0 to compress prompts before Tier 3 calls
7. **MCP Pointer**: Specific browser automation tool (not generic Browser-Use)

**Cost Optimization Enhancements**:
- Pre-flight budget checks prevent overruns
- Tier 0 fast filter reduces overall costs by 40%
- Batch detection saves 30-50% on high-volume workloads
- Architect/executor pattern saves 50-60% on multi-step tasks
- Prompt compression reduces Tier 3 token usage by 60%

**Observability Enhancements**:
- Routing rationale logged (complexity, modality, batch size, cost)
- Tier distribution tracked in real-time
- Budget utilization alerts at 80%, 90%, 100%
- Screenshot audit trail for web agent debugging

**New Agents**:
- **Coder Agent**: Architect/executor pattern with tier optimization
- **Web Agent**: MCP Pointer + Firecrawl + vision models

**Expected Cost Reduction**: 60-70% vs naive Tier 3 for everything
