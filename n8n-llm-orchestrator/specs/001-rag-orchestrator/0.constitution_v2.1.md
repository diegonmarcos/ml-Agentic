<!--
Sync Impact Report
==================
Version: 2.0.0 → 2.1.0 (Multi-Modal Agent Orchestrator + Cost Orchestration)
Modified Principles: II (LLM-Agnostic extended with modality), IV (Cost-Conscious extended with pricing models)
Added Sections: VIII (Cost-Optimized Provider Selection), Web Agent Standards, Provider Pricing Matrix
Breaking Changes:
  - New Cost Orchestration Engine required
  - Provider pricing configuration mandatory
  - Modality detection in routing layer
  - Web agent capabilities (Browser-Use/Playwright MCP)
Templates Status: ✅ Requires update to spec, plan, research, and tasks
Follow-up TODOs:
  - Deploy Cost Orchestration Engine
  - Configure provider pricing matrix (hourly vs per-token)
  - Integrate Browser-Use or Playwright MCP
  - Set up RunPod, Salad, CloudRift, Fireworks, Together.ai accounts
  - Implement workload analyzer for volume-based routing
  - Add vision model routing (Llama 3.2 Vision, Claude 3.5, GPT-4V)
-->

# n8n Multi-Modal Agent Orchestrator Constitution

**Version**: 2.1.0
**Capabilities**: Document RAG, Web Agent, Multi-Tier Cost Orchestration
**Ratified**: 2025-10-30
**Last Amended**: 2025-11-17

---

## Core Principles

### I. Visual-First Design
All agentic workflows MUST be representable in n8n's visual interface. This ensures:
- Non-technical stakeholders can understand agent flows at a glance
- Debugging is visual and intuitive
- No hidden logic in external scripts unless absolutely necessary for performance
- Complex multi-agent orchestrations remain maintainable
- **Web agent tasks**: Browser automation steps visible as n8n nodes

**Rationale**: Visual workflows reduce cognitive load and make agentic systems accessible to broader teams.

### II. LLM-Agnostic & Modality-Aware Architecture
The system MUST support multiple LLM providers interchangeably with modality awareness:

**Supported Providers**:
- OpenAI (GPT-4, GPT-4 Turbo, GPT-4o) - Text + Vision
- Anthropic (Claude 3.5 Sonnet, Claude 3 Opus) - Text + Vision
- Google (Gemini 1.5 Pro, Gemini Flash) - Text + Vision + Multimodal
- Ollama (local models: Llama 3.1, Mistral, Qwen) - Text-only
- Fireworks.ai (Llama 3.1 70B, Mixtral) - Text-only
- Together.ai (Llama 3.2 Vision, Mixtral 8x22B) - Text + Vision
- RunPod (custom models) - Text + Vision (hourly VRAM)
- Salad (Llama 3.1 70B) - Text-only (hourly VRAM)
- CloudRift (custom models) - Text + Vision (hourly VRAM)

**Requirements**:
- ALL LLM calls MUST go through unified interface layer (**aisuite**)
- Provider switching MUST NOT require workflow redesign
- Each agent can specify preferred provider AND modality requirements (text/vision/multimodal)
- Fallback chains supported (e.g., Ollama → Fireworks → Claude → GPT-4)
- n8n workflows interact with aisuite HTTP API, not individual providers
- **Modality detection**: Automatically select vision-capable models for screenshot analysis tasks

**Rationale**: Avoid vendor lock-in, enable cost optimization across pricing models, support offline/local operation. Single API interface reduces n8n complexity. Modality awareness ensures correct model selection for vision tasks.

### III. Fail-Safe Operations (NON-NEGOTIABLE)
Every LLM call, tool execution, data retrieval, and **browser action** MUST implement error handling:
- Automatic retries with exponential backoff (max 3 attempts)
- Graceful degradation when services unavailable
- Meaningful error messages returned to user
- Circuit breaker pattern for repeated failures
- No workflow crash—always return actionable response
- **Web agent**: Browser failures trigger retry with alternative strategy or human escalation
- **Provider failures**: Hourly VRAM instance startup failures fallback to per-token providers

**Rationale**: LLM APIs and browser automation are inherently unreliable; production systems require resilience.

### IV. Cost-Conscious Design
Token usage and API costs MUST be actively managed across multiple pricing models:
- Monitor and log token consumption per workflow via **Helicone** proxy
- Track real-time costs with **MCPcat** production monitoring
- Implement prompt optimization strategies (chunking, compression)
- Cache LLM responses for identical queries via **Helicone** (1-hour TTL)
- Prefer local Ollama for development and high-volume text-only use cases
- Use **RouteLLM** to automatically route simple queries to cheaper models
- Track monthly spend with budget alerts via **LangSmith** or **Langfuse**
- **NEW**: Route high-volume workloads (>100 queries/hour) to hourly VRAM providers (RunPod, Salad)
- **NEW**: Route low-volume workloads (<20 queries/hour) to per-token providers (Fireworks, Together.ai)
- **NEW**: Shutdown idle hourly instances after 30 minutes to prevent waste
- **NEW**: Maintain separate budget pools for hourly, per-token, and premium APIs

**Rationale**: Unchecked LLM usage can lead to unexpected costs; proactive monitoring with professional tools prevents budget overruns. Intelligent routing optimizes cost-quality tradeoff automatically. Different pricing models (hourly VRAM vs per-token) require different optimization strategies.

### V. Observable Agent Flows
Every agent step MUST be traceable and debuggable:
- Distributed tracing with **OpenTelemetry** across n8n → aisuite → providers
- Structured logging with context (agent name, step, timestamp) via **MCPcat**
- Capture all LLM prompts and responses in **LangSmith** or **Langfuse**
- Track RAG retrieval results (chunks retrieved, relevance scores)
- Execution metrics (latency, token count, success rate) via **Helicone**
- Full request traces with visualization in **LangSmith** dashboard
- n8n execution logs retained for 30 days minimum
- **NEW**: Log all browser actions (navigate, click, type, screenshot) to audit trail
- **NEW**: Capture screenshots at each web agent step for visual debugging
- **NEW**: Log provider selection rationale (complexity, modality, volume, cost)

**Rationale**: Complex multi-agent systems are opaque without comprehensive observability. Professional tools provide trace visualization and debugging capabilities that simple logging cannot match. Web agents require visual debugging via screenshots.

### VI. Intelligent Routing
Query complexity MUST determine model selection automatically:
- **RouteLLM** analyzes queries and routes to appropriate model tier
- Simple queries (factual lookup, formatting) → Local Ollama or Gemini Flash
- Medium queries (reasoning, multi-step) → Claude 3.5 Sonnet or GPT-4
- Complex queries (advanced reasoning, coding) → Claude 3 Opus or GPT-4 Turbo
- Routing decisions logged and auditable
- Manual override available per workflow via configuration
- **NEW**: Modality awareness integrated (text-only vs vision vs multimodal)

**Rationale**: Manual model selection wastes time and money. Intelligent routing optimizes cost-quality tradeoff while maintaining answer quality. Reduces cognitive load on developers.

### VII. Tool Integration Standards
External tools and data sources MUST follow MCP (Model Context Protocol) patterns:
- **Web scraping (read-only)**: Use **Firecrawl** for LLM-friendly structured output
- **Web automation (read-write)**: Use **Browser-Use** or **Playwright MCP** for browser control
- **Documentation access**: Use **PageIndex MCP** for vectorless RAG (cost-efficient)
- **App integrations** (future): Use **Composio** or **Rube** for standardized API access
- **Authentication** (future): Use **AgentAuth** for OAuth flows and token management
- All tools MUST expose consistent interfaces via n8n HTTP Request nodes
- Tool failures MUST NOT crash workflows (fallback to degraded functionality)

**Rationale**: Standardized tool integration reduces maintenance burden and enables tool swapping. MCP provides proven patterns for context management.

### VIII. Cost-Optimized Provider Selection (NEW)

Provider orchestration MUST consider both model capability AND pricing model economics:

#### Pricing Model Categories

1. **Local (Free)**:
   - **Providers**: Ollama
   - **Cost**: $0 per query
   - **Use When**: Development, high-volume text-only queries, offline scenarios
   - **Limitation**: Text-only, limited model sizes, requires local GPU

2. **Hourly VRAM Providers** (RunPod, Salad, CloudRift):
   - **Cost**: $0.30-1.20 per hour (fixed, regardless of query count)
   - **Best For**: High-volume workloads (>100 queries/hour), batch processing, sustained load
   - **Strategy**: Keep instance warm when volume threshold met, shutdown after 30 min idle
   - **Economics**: Cost-effective when amortized over many queries
   - **Startup Time**: 1-3 minutes (cold start penalty)

3. **Per-Token Providers** (Fireworks, Together.ai, Replicate):
   - **Cost**: $0.10-0.30 per million tokens (metered usage)
   - **Best For**: Bursty traffic, low-volume queries (<20/hour), sporadic usage
   - **Strategy**: Use on-demand, no idle cost
   - **Economics**: Pay only for actual usage, no instance management
   - **Startup Time**: Instant (no cold start)

4. **Premium APIs** (Claude, GPT-4, Gemini):
   - **Cost**: $3-15 per million tokens input, $10-75 per million tokens output
   - **Best For**: Complex reasoning, highest quality requirements, vision tasks
   - **Strategy**: Reserve for top-tier queries only
   - **Economics**: Expensive but highest quality

#### Routing Logic

**Step 1: Detect Modality Requirements**
```
if task requires vision:
    eligible = [Together_Llama32V, Claude35, GPT4V, Gemini15Pro, CloudRift]
elif task requires text only:
    eligible = [Ollama, Fireworks, RunPod, Salad, Claude35]
```

**Step 2: Analyze Complexity (RouteLLM)**
```
complexity_score = RouteLLM.analyze(query)
if complexity < 0.3:  # Simple
    tier = [Ollama, Fireworks, Gemini_Flash]
elif complexity < 0.7:  # Medium
    tier = [RunPod, Together, Claude35]
else:  # Complex
    tier = [Claude_Opus, GPT4_Turbo]
```

**Step 3: Check Current Workload Volume**
```
queries_per_hour = get_recent_volume(last_60_minutes)

if queries_per_hour > 100:  # High volume
    # Prefer hourly VRAM (amortize cost)
    preferred = [RunPod, Salad, CloudRift]
    if not instance_running(preferred):
        start_instance_async(preferred)  # 1-3 min startup
        fallback_to_per_token()  # Use while starting
elif queries_per_hour < 20:  # Low volume
    # Prefer per-token (no idle cost)
    preferred = [Fireworks, Together]
    shutdown_idle_hourly_instances()  # Save cost
else:  # Medium volume
    # Hybrid: local + per-token
    preferred = [Ollama, Fireworks]
```

**Step 4: Select from Intersection**
```
final_provider = intersect(eligible, tier, preferred)[0]
log_selection_rationale(query, complexity, modality, volume, final_provider, cost_projection)
```

#### Budget Pool Management

**Monthly Budget Allocation**:
- **Pool A (Hourly VRAM)**: $100/month
  - RunPod: Up to 170 hours/month ($0.59/hour)
  - Salad: Up to 220 hours/month ($0.45/hour)
  - CloudRift: Up to 112 hours/month ($0.89/hour)

- **Pool B (Per-Token)**: $50/month
  - Fireworks: ~250K queries at $0.20/M tokens
  - Together.ai: ~275K queries at $0.18/M tokens

- **Pool C (Premium APIs)**: $30/month
  - Claude/GPT-4: ~500-1000 queries depending on length

**Alerts**:
- Hourly instance idle >30 minutes → Auto-shutdown alert
- Per-token spend >$10/day → Downgrade to local Ollama alert
- Premium API spend >$25/month → Route via RouteLLM more aggressively
- Any pool >90% capacity → Budget warning

#### Instance Lifecycle Management

**Hourly VRAM Provider Lifecycle**:
1. **Startup Trigger**: When predicted queries in next hour >50
2. **Keep Warm**: While queries/hour >100 OR queries in last 30 min >20
3. **Shutdown**: After 30 minutes of idle (<5 queries in 30 min)
4. **Cold Start Handling**: Fallback to per-token providers during 1-3 min startup

**Rationale**: Different pricing models require different optimization strategies. Hourly VRAM providers are cheaper for sustained high-volume workloads when amortized over many queries. Per-token providers are cheaper for bursty, low-volume traffic. System MUST dynamically route based on current workload patterns and projected costs.

---

## Architecture Constraints

### Vector Database Standards
- **Primary**: Qdrant (self-hosted) for production RAG workflows with embeddings
- **Alternative (Cost-Optimized)**: PageIndex MCP for vectorless RAG on documentation
- **Alternative (Cloud)**: Supabase Vector for cloud-hosted scenarios
- All embeddings MUST use consistent model (nomic-embed-text, 768 dimensions)
- Collections MUST include metadata: source, timestamp, chunk_id, version
- Backup collections daily via Qdrant snapshot API
- Use PageIndex MCP for frequently-accessed documentation to reduce embedding costs

### Document Processing Rules
- Chunk size: 800 characters (±200 acceptable for specific use cases)
- Chunk overlap: 200 characters minimum
- Top-k retrieval: 3-5 chunks (never exceed 10 to prevent context overflow)
- Re-ranking optional but recommended for precision-critical applications

### State Management
- Agent state MUST be passed explicitly between n8n nodes (no hidden globals)
- Long-running workflows MUST persist state to prevent data loss
- Maximum workflow execution time: 5 minutes (configurable per workflow)
- **Web agent**: Browser state (cookies, session) persisted between steps

### LLM Interface & Routing Layer
- **Unified API**: All LLM calls routed through **aisuite** HTTP wrapper service
- **Intelligent Routing**: **RouteLLM** determines optimal model based on query complexity
- **Cost Orchestration**: **Cost Orchestrator** selects provider based on workload volume and pricing model
- **Cost Proxy**: **Helicone** sits between aisuite and providers for caching/logging
- **Request Flow**: n8n → Cost Orchestrator (volume analysis) → RouteLLM (complexity) → aisuite → Helicone → Provider
- **Configuration**: Model preferences, routing thresholds, fallback chains in environment variables
- **Override**: Manual model selection available via workflow parameters

### Web Agent Standards (NEW)
- **Browser Automation**: Browser-Use (preferred) or Playwright MCP for browser control
- **Supported Actions**: navigate, click, type, select, submit, screenshot, scroll, wait
- **State Management**: Cookies, localStorage, sessionStorage persisted between steps
- **Error Handling**: Retry with exponential backoff, alternative selectors, human escalation
- **Visual Understanding**: Screenshot analysis via vision-capable LLM (Claude 3.5, GPT-4V, Llama 3.2 Vision)
- **Multi-Step Tasks**: Task decomposition, state persistence, resume on failure
- **Security**: No CAPTCHA bypass (human escalation), respect robots.txt, rate limiting
- **Audit Trail**: All actions logged with screenshots to MCPcat + LangSmith

### Observability Stack (Required for Production)
- **Distributed Tracing**: OpenTelemetry with Jaeger/Zipkin backend
- **LLM Monitoring**: LangSmith (preferred) or Langfuse (opensource alternative)
- **Cost Tracking**: Helicone proxy with real-time dashboard + Cost Orchestrator logs
- **Agent Metrics**: MCPcat for tool call tracking and error monitoring
- **Integration**: All tools export traces to OpenTelemetry collector
- **Retention**: 30 days for execution logs, 90 days for aggregated metrics
- **Alerting**: Budget alerts, error rate thresholds, latency spikes, idle instance warnings

---

## Quality Standards

### Testing Requirements
- All RAG pipelines MUST have evaluation datasets with ground truth
- Measure and track: retrieval accuracy (recall@k), answer relevance, latency
- Integration tests required for: document ingestion, query workflows, fallback paths, **web agent tasks**
- Manual testing protocol documented for each workflow before production
- **Web agent**: Test on 10+ diverse websites (e-commerce, news, documentation, forms)
- **Cost orchestration**: Simulate high-volume and low-volume workloads, verify correct routing

### Performance Benchmarks
- RAG query response time: < 5 seconds end-to-end
- Document ingestion: > 10 docs/minute
- Concurrent workflow support: minimum 5 simultaneous executions
- LLM response timeout: 30 seconds (configurable)
- **Web agent**: < 30 seconds for simple tasks (<5 steps), < 2 minutes for complex tasks
- **Provider startup**: Hourly VRAM instances start within 3 minutes
- **Cost routing**: Decision time < 100ms (negligible overhead)

### Security
- API keys stored in n8n credentials (never hardcoded)
- Vector database access restricted to local network or VPN
- User input sanitized before LLM prompts
- No PII in logs unless explicitly required and compliant
- **Web agent**: No CAPTCHA bypass, respect robots.txt, rate limiting on target sites
- **Provider security**: Separate credentials per provider, least-privilege access

---

## Governance

This constitution supersedes all other development practices. Changes require:
1. Proposal with rationale and impact analysis
2. Version bump following semantic versioning (MAJOR.MINOR.PATCH)
3. Update to all dependent templates and documentation
4. Team review (or author review for solo projects)

All workflows and implementations MUST be verified against these principles during:
- Design review (before implementation)
- Code/workflow review (before deployment)
- Post-deployment audit (after 1 week in production)

Complexity that violates these principles MUST be explicitly justified and documented as technical debt with remediation plan.

**Version**: 2.1.0 | **Ratified**: 2025-10-30 | **Last Amended**: 2025-11-17
